# -*- coding: utf-8 -*-
"""機械学習アプリ_画像分類.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_9UIVhuCGNkTxj4eJ1z-6k6e37WuWy-U
"""

!pip install -q pytorch_lightning
!pip install -q torchmetrics
!pip install icrawler

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision
from torchvision import transforms
from torchvision import datasets
import pytorch_lightning as pl
import torchmetrics
from torchmetrics.functional import accuracy
import torchsummary
from torchsummary import summary
from pytorch_lightning.loggers import CSVLogger
from PIL import Image
from PIL import ImageFile

# ダウンロード後の処理について定義
transform = transforms.Compose([
    transforms.Resize((256, 256)),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

# 画像のダウンロード
from icrawler.builtin import GoogleImageCrawler, BingImageCrawler
# PILは、大きなサイズのデータをデフォルトで見過ごす仕様になっているので、True(見過ごさない)設定にする。
ImageFile.LOAD_TRUNCATED_IMAGES = True

# ホッキョクグマ
# storage : 保存場所及びディレクトリの指定。パスを指定すれば任意の場所に画像を格納可能
# google_crawler1 = GoogleImageCrawler(parser_threads=4, storage={'root_dir': '/content/animal/train/polar_bear'})
# google_crawler1.crawl(keyword='ホッキョクグマ', max_num=120)
# bing_crawler1 = BingImageCrawler(parser_threads=4, storage={'root_dir': '/content/animal/train/polar_bear'})
# bing_crawler1.crawl(keyword='ホッキョクグマ', filters=None, max_num=130)
# ライオン
# google_crawler2 = GoogleImageCrawler(parser_threads=4, storage={'root_dir': '/content/animal/train/lion'})
# google_crawler2.crawl(keyword='ライオン', max_num=130)
# bing_crawler2 = BingImageCrawler(parser_threads=4, storage={'root_dir': '/content/animal/train/lion'})
# bing_crawler2.crawl(keyword='ライオン', filters=None, max_num=130)
# シマウマ
# google_crawler3 = GoogleImageCrawler(parser_threads=4, storage={'root_dir': '/content/animal/train/zebra'})
# google_crawler3.crawl(keyword='シマウマ', max_num=120)
# bing_crawler3 = BingImageCrawler(parser_threads=4, storage={'root_dir': '/content/animal/train/zebra'})
# bing_crawler3.crawl(keyword='シマウマ', filters=None, max_num=130)
# ゾウ
# google_crawler4 = GoogleImageCrawler(parser_threads=4, storage={'root_dir': '/content/animal/train/elephant'})
# google_crawler4.crawl(keyword='ゾウ', max_num=130)
# bing_crawler4 = BingImageCrawler(parser_threads=4, storage={'root_dir': '/content/animal/train/elephant'})
# bing_crawler4.crawl(keyword='ゾウ', filters=None, max_num=130)

# ゾウの画像を表示
from IPython.display import Image,display_jpeg
display_jpeg(Image("/content/animal/train/elephant/000001.jpg"))

# 隠しファイルの.ipynb_checkpointsがある場合は、.class_to_idxする際に"0"として出力されるので、以下コードで削除する。
# !rm -rf animal/.ipynb_checkpoints

dataset = datasets.ImageFolder('/content/animal/train/', transform)

dataset

"""前処理後の画像を確認する為に、(縦, 横, チャネル数)に変換する。"""

dataset[0][0].shape

dataset_sample = np.transpose(dataset[0][0], (1, 2, 0))

dataset_sample.shape

plt.imshow(dataset_sample)

# クラスの確認
dataset.class_to_idx

# データの分割
train_val = dataset

# 分割割合
n_train = 360
n_val = 98

# シードの固定
pl.seed_everything(0)

# データの分割
train, val = torch.utils.data.random_split(train_val, [n_train, n_val])

# バッチサイズ
batch_size = 10

# DataLoader
train_loader = torch.utils.data.DataLoader(train, batch_size, shuffle=True, drop_last=True)
val_loader = torch.utils.data.DataLoader(val, batch_size)

# ファインチューニング
from torchvision.models import resnet18

# ネットワークの構築
class Net(pl.LightningModule):
    def __init__(self):
        super().__init__()

        self.feature = resnet18(pretrained=True)
        self.fc = nn.Linear(1000, 4)

    def forward(self, x):
        h = self.feature(x)
        h = self.fc(h)
        return h

    def training_step(self, batch, batch_idx):
        x, t = batch
        y = self(x)
        loss = F.cross_entropy(y, t)
        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True)
        self.log('train_acc', accuracy(y.softmax(dim=-1), t), on_step=True, on_epoch=True, prog_bar=True)
        return loss

    def validation_step(self, batch, batch_idx):
        x, t = batch
        y = self(x)
        loss = F.cross_entropy(y, t)
        self.log('val_loss', loss, on_step=False, on_epoch=True)
        self.log('val_acc', accuracy(y.softmax(dim=-1), t), on_step=False, on_epoch=True)
        return loss

    # def test_step(self, batch, batch_idx):
    #     x, t = batch
    #     y = self(x)
    #     loss = F.cross_entropy(y, t)
    #     self.log('test_loss', loss, on_step=False, on_epoch=True)
    #     self.log('test_acc', accuracy(y.softmax(dim=-1), t), on_step=False, on_epoch=True)
    #     return loss

    def configure_optimizers(self):
        optimizer = torch.optim.SGD(self.parameters(), lr=0.01)
        return optimizer

# 学習
pl.seed_everything(0)
net = Net()
logger = CSVLogger(save_dir='logs', name='my_exp')
trainer = pl.Trainer(max_epochs=5, gpus=1, deterministic=True, logger=logger)
trainer.fit(net, train_loader, val_loader)

# 学習結果
trainer.callback_metrics

# 学習したモデルの保存
weight = net.state_dict()
torch.save(weight, 'animal.pt')

# ネットワークの構築
class Net(pl.LightningModule):
    def __init__(self):
        super().__init__()

        self.feature = resnet18(pretrained=True)
        self.fc = nn.Linear(1000, 4)

    def forward(self, x):
        h = self.feature(x)
        h = self.fc(h)
        return h

# インスタンス化
net = Net().eval()

# 重みの読み込み
net.load_state_dict(torch.load('animal.pt'))

# テスト画像
test_img = transform(Image.open('/content/animal/test/test3.jpg'))

test_img

test_img = test_img.unsqueeze(0)
test_img

with torch.no_grad():
    y = net(test_img)
    y_proba = y.softmax(dim=-1)
    y = torch.argmax(y, dim=1)

print('確率:', np.array(y_proba.max()*100))
print('どの動物か:', np.array(y[0]))

"""それぞれの中身の大小を比較して、その結果に応じて分類すると同時に、その動物である確率を取得する。"""

key = [k for k, v in dataset.class_to_idx.items() if v == y][0]
print(key)

